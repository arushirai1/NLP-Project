{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f33451d5-4d7d-470f-937d-456f4a14df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Dataset\n",
    "importlib.reload(Dataset)\n",
    "from Dataset import MAMIDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c8c5672-fb59-4249-8335-0e6ad0d08ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837e777e3cae427e9470039ff0c6ef32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237a6c89011e4e2090391a43bca4b806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b1540efb5841cf82f7ed23191adbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83709aec0284261b872f7466b0ed3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7926 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arushirai/Downloads/Notebooks_export/NLP-Project/Dataset.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['indexed_tokens'] = df.tokens.progress_apply(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba055e1265e34d52a13b35c38be21549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7926 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arushirai/Downloads/Notebooks_export/NLP-Project/Dataset.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['bow_vector'] = df.indexed_tokens.progress_apply(\n",
      "/Users/arushirai/Downloads/Notebooks_export/NLP-Project/Dataset.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vectors = vectorizer.fit_transform(df.tokens).toarray()\n"
     ]
    }
   ],
   "source": [
    "from params import *\n",
    "train_dataset = MAMIDataset(MAX_LEN,MAX_VOCAB, split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3441179b-b727-44fe-862a-75e1225cddfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('women', 752),\n",
       " ('like', 639),\n",
       " ('imgflip.com', 528),\n",
       " (\"i'm\", 475),\n",
       " ('woman', 457),\n",
       " ('get', 440),\n",
       " ('men', 327),\n",
       " ('-', 314),\n",
       " ('wife', 300),\n",
       " ('one', 295)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# all words\n",
    "all_words=[]\n",
    "[all_words.extend([token.lower() for token in i.split() if token.lower() not in train_dataset.stop_words]) for i in train_dataset.text]\n",
    "Counter(all_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46fd76f4-a0de-4f26-b5bb-2d91afaa2278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a' in train_dataset.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ad92aac-7630-42ec-a0e2-da7213a0b373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7926it [00:00, 192392.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('women', 606),\n",
       " ('like', 383),\n",
       " ('woman', 304),\n",
       " ('imgflip.com', 233),\n",
       " ('men', 229),\n",
       " ('get', 223),\n",
       " ('girls', 214),\n",
       " ('girl', 207),\n",
       " (\"i'm\", 199),\n",
       " ('quickmeme.com', 192)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "misogyny_words=[]\n",
    "pb = tqdm(enumerate(train_dataset.text))\n",
    "for i, text in pb:\n",
    "    if train_dataset.label_arr[i][0] == 1:\n",
    "        misogyny_words.extend([token.lower() for token in text.split() if token.lower() not in train_dataset.stop_words])\n",
    "        \n",
    "Counter(misogyny_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb440753-82a3-4132-8605-06c7c13c14b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7926it [00:00, 111657.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('imgflip.com', 295),\n",
       " (\"i'm\", 276),\n",
       " ('like', 256),\n",
       " ('girlfriend', 226),\n",
       " ('call', 224),\n",
       " ('house', 217),\n",
       " ('get', 217),\n",
       " ('me:', 211),\n",
       " ('wife', 190),\n",
       " ('one', 173)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "non_misogyny_words=[]\n",
    "pb = tqdm(enumerate(train_dataset.text))\n",
    "for i, text in pb:\n",
    "    if train_dataset.label_arr[i][0] == 0:\n",
    "        non_misogyny_words.extend([token.lower() for token in text.split() if token.lower() not in train_dataset.stop_words])\n",
    "        \n",
    "Counter(non_misogyny_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bb2fa9b-2272-4feb-88d1-5a5b0138c61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7926"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b00f1d5-3034-4d6e-9bbc-1d6d6fa9e4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7926"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec5cf52d-df7a-495d-972f-9cb686736f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_misogyny_words_set=set(list(zip(*Counter(non_misogyny_words).most_common(50)))[0])\n",
    "misogyny_words_set=set(list(zip(*Counter(misogyny_words).most_common(50)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f109e05f-96d7-4981-b65b-c08d92096613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-',\n",
       " \"can't\",\n",
       " 'female',\n",
       " 'first',\n",
       " 'get',\n",
       " 'go',\n",
       " 'good',\n",
       " 'got',\n",
       " 'hooker',\n",
       " \"i'm\",\n",
       " 'imgflip.com',\n",
       " 'kitchen',\n",
       " 'know',\n",
       " 'like',\n",
       " 'made',\n",
       " 'make',\n",
       " 'man',\n",
       " 'men',\n",
       " 'never',\n",
       " 'new',\n",
       " 'one',\n",
       " 'see',\n",
       " 'think',\n",
       " 'time',\n",
       " 'want',\n",
       " 'wife',\n",
       " 'woman',\n",
       " 'women'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_misogyny_words_set.intersection(misogyny_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ed25f2d-fc4a-436b-a7dd-f9d3d3a712d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4002 3924\n"
     ]
    }
   ],
   "source": [
    "# getting counts per class\n",
    "m_count=sum([int([i][0][0]) for i in train_dataset.label_arr])\n",
    "nm_count=len(train_dataset.label_arr) - m_count\n",
    "print(m_count, nm_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac76d15-9ca0-4ffd-bdbc-f926cf2a5603",
   "metadata": {},
   "source": [
    "# Get examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f113e09c-08d5-4a06-b5ae-13585a504089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/arushirai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/arushirai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from analysis_script import TextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cfb5c5a-8aac-4cad-a1c6-12b77281519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the part of speech\n",
      "Finished vectorizing the part of speech\n",
      "Average sentence length:  22.2778\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, namedtuple\n",
    "\n",
    "args=namedtuple(typename='test', field_names=['data_path', 'dataset'])\n",
    "args.data_path='./Data/TRAINING/training.csv'\n",
    "args.dataset='mami'\n",
    "dataset=TextDataset(path_to_data=args.data_path, dataset_name=args.dataset)\n",
    "    \n",
    "# get average sentence length\n",
    "print(\"Average sentence length: \", dataset.get_average_sentence_length(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bccfe31-cda0-46d2-857f-84b38088996b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>misogynous</th>\n",
       "      <th>shaming</th>\n",
       "      <th>stereotype</th>\n",
       "      <th>objectification</th>\n",
       "      <th>violence</th>\n",
       "      <th>Text Transcription</th>\n",
       "      <th>text</th>\n",
       "      <th>sentence_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10007.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PICTOPHLE APP *Straight white malle starts tal...</td>\n",
       "      <td>[PICTOPHLE, APP, *, Straight, white, malle, st...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10032.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>maia @mxmtoon stop calling women \"females,\" it...</td>\n",
       "      <td>[maia, @, mxmtoon, stop, calling, women, \", fe...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>10134.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>maxell - Polaroi Trous IVE JUST FOUND MY CHRIS...</td>\n",
       "      <td>[maxell, -, Polaroi, Trous, IVE, JUST, FOUND, ...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>10139.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Looking at the map for some weekend travel ide...</td>\n",
       "      <td>[Looking, at, the, map, for, some, weekend, tr...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>10141.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Julie Bindel âœ” @bindelj - Dec 19, 2019 What ...</td>\n",
       "      <td>[Julie, Bindel, âœ, ”, @, bindelj, -, Dec, 19,...</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9238</th>\n",
       "      <td>921.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The virgin \"wholesome meme\" literally a pixela...</td>\n",
       "      <td>[The, virgin, \", wholesome, meme, \", literally...</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9260</th>\n",
       "      <td>9234.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>*le me donating to charity \"You bragging donat...</td>\n",
       "      <td>[*, le, me, donating, to, charity, \", You, bra...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9665</th>\n",
       "      <td>9662.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18:31 9 Post Nice try Satan!! Guys, can you su...</td>\n",
       "      <td>[18, :, 31, 9, Post, Nice, try, Satan, !!, Guy...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9947</th>\n",
       "      <td>9949.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SFR 11:00 When I watch anime with my french we...</td>\n",
       "      <td>[SFR, 11, :, 00, When, I, watch, anime, with, ...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9969</th>\n",
       "      <td>9973.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>but.. but... how?? I dont remember shit! Just ...</td>\n",
       "      <td>[but, .., but, ..., how, ??, I, dont, remember...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_name  misogynous  shaming  stereotype  objectification  violence  \\\n",
       "5     10007.jpg           0        0           0                0         0   \n",
       "27    10032.jpg           1        1           0                0         0   \n",
       "117   10134.jpg           0        0           0                0         0   \n",
       "122   10139.jpg           0        0           0                0         0   \n",
       "125   10141.jpg           0        0           0                0         0   \n",
       "...         ...         ...      ...         ...              ...       ...   \n",
       "9238    921.jpg           0        0           0                0         0   \n",
       "9260   9234.jpg           0        0           0                0         0   \n",
       "9665   9662.jpg           0        0           0                0         0   \n",
       "9947   9949.jpg           0        0           0                0         0   \n",
       "9969   9973.jpg           0        0           0                0         0   \n",
       "\n",
       "                                     Text Transcription  \\\n",
       "5     PICTOPHLE APP *Straight white malle starts tal...   \n",
       "27    maia @mxmtoon stop calling women \"females,\" it...   \n",
       "117   maxell - Polaroi Trous IVE JUST FOUND MY CHRIS...   \n",
       "122   Looking at the map for some weekend travel ide...   \n",
       "125   Julie Bindel âœ” @bindelj - Dec 19, 2019 What ...   \n",
       "...                                                 ...   \n",
       "9238  The virgin \"wholesome meme\" literally a pixela...   \n",
       "9260  *le me donating to charity \"You bragging donat...   \n",
       "9665  18:31 9 Post Nice try Satan!! Guys, can you su...   \n",
       "9947  SFR 11:00 When I watch anime with my french we...   \n",
       "9969  but.. but... how?? I dont remember shit! Just ...   \n",
       "\n",
       "                                                   text  sentence_counts  \n",
       "5     [PICTOPHLE, APP, *, Straight, white, malle, st...              130  \n",
       "27    [maia, @, mxmtoon, stop, calling, women, \", fe...              111  \n",
       "117   [maxell, -, Polaroi, Trous, IVE, JUST, FOUND, ...              103  \n",
       "122   [Looking, at, the, map, for, some, weekend, tr...              101  \n",
       "125   [Julie, Bindel, âœ, ”, @, bindelj, -, Dec, 19,...              316  \n",
       "...                                                 ...              ...  \n",
       "9238  [The, virgin, \", wholesome, meme, \", literally...              297  \n",
       "9260  [*, le, me, donating, to, charity, \", You, bra...              155  \n",
       "9665  [18, :, 31, 9, Post, Nice, try, Satan, !!, Guy...              107  \n",
       "9947  [SFR, 11, :, 00, When, I, watch, anime, with, ...              107  \n",
       "9969  [but, .., but, ..., how, ??, I, dont, remember...              101  \n",
       "\n",
       "[104 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df[dataset.df['sentence_counts']>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed677871-2d5c-4104-955b-cb9c60ce5a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (untitled)",
   "language": "python",
   "name": "pycharm-4b0cbf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
